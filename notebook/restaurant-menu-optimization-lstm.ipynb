{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1789966,"sourceType":"datasetVersion","datasetId":1063627}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Summary : Food Rating Prediction For Menu Optimization\n\nThis project predicts food recipe ratings using a combination of text analysis and numerical features. It processes user reviews with NLP techniques (LSTM), capturing the semantic meaning of reviews, and integrates recipe details like cooking time and nutritional content. The model is trained to predict ratings, helping optimize menus by recommending high-rated recipes. The project demonstrates the application of deep learning and text processing to predict user ratings in the food domain, leveraging both the textual sentiment and content of reviews for more accurate predictions.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T06:09:19.057748Z","iopub.execute_input":"2024-12-10T06:09:19.058107Z","iopub.status.idle":"2024-12-10T06:09:19.903452Z","shell.execute_reply.started":"2024-12-10T06:09:19.058056Z","shell.execute_reply":"2024-12-10T06:09:19.902325Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/foodcom-recipes-and-reviews/recipes.parquet\n/kaggle/input/foodcom-recipes-and-reviews/reviews.parquet\n/kaggle/input/foodcom-recipes-and-reviews/reviews.csv\n/kaggle/input/foodcom-recipes-and-reviews/recipes.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## Loading the Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the reviews and recipes data\nreviews_df = pd.read_csv('/kaggle/input/foodcom-recipes-and-reviews/reviews.csv')\nrecipes_df = pd.read_csv('/kaggle/input/foodcom-recipes-and-reviews/recipes.csv')\n\n# Display basic info about the datasets\nprint(reviews_df.info())\nprint(recipes_df.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T06:09:19.905194Z","iopub.execute_input":"2024-12-10T06:09:19.905689Z","iopub.status.idle":"2024-12-10T06:09:50.169273Z","shell.execute_reply.started":"2024-12-10T06:09:19.905651Z","shell.execute_reply":"2024-12-10T06:09:50.168436Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1401982 entries, 0 to 1401981\nData columns (total 8 columns):\n #   Column         Non-Null Count    Dtype \n---  ------         --------------    ----- \n 0   ReviewId       1401982 non-null  int64 \n 1   RecipeId       1401982 non-null  int64 \n 2   AuthorId       1401982 non-null  int64 \n 3   AuthorName     1401982 non-null  object\n 4   Rating         1401982 non-null  int64 \n 5   Review         1401768 non-null  object\n 6   DateSubmitted  1401982 non-null  object\n 7   DateModified   1401982 non-null  object\ndtypes: int64(4), object(4)\nmemory usage: 85.6+ MB\nNone\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 522517 entries, 0 to 522516\nData columns (total 28 columns):\n #   Column                      Non-Null Count   Dtype  \n---  ------                      --------------   -----  \n 0   RecipeId                    522517 non-null  int64  \n 1   Name                        522517 non-null  object \n 2   AuthorId                    522517 non-null  int64  \n 3   AuthorName                  522517 non-null  object \n 4   CookTime                    439972 non-null  object \n 5   PrepTime                    522517 non-null  object \n 6   TotalTime                   522517 non-null  object \n 7   DatePublished               522517 non-null  object \n 8   Description                 522512 non-null  object \n 9   Images                      522516 non-null  object \n 10  RecipeCategory              521766 non-null  object \n 11  Keywords                    505280 non-null  object \n 12  RecipeIngredientQuantities  522514 non-null  object \n 13  RecipeIngredientParts       522517 non-null  object \n 14  AggregatedRating            269294 non-null  float64\n 15  ReviewCount                 275028 non-null  float64\n 16  Calories                    522517 non-null  float64\n 17  FatContent                  522517 non-null  float64\n 18  SaturatedFatContent         522517 non-null  float64\n 19  CholesterolContent          522517 non-null  float64\n 20  SodiumContent               522517 non-null  float64\n 21  CarbohydrateContent         522517 non-null  float64\n 22  FiberContent                522517 non-null  float64\n 23  SugarContent                522517 non-null  float64\n 24  ProteinContent              522517 non-null  float64\n 25  RecipeServings              339606 non-null  float64\n 26  RecipeYield                 174446 non-null  object \n 27  RecipeInstructions          522517 non-null  object \ndtypes: float64(12), int64(2), object(14)\nmemory usage: 111.6+ MB\nNone\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Merge Reviews with Recipes","metadata":{}},{"cell_type":"code","source":"# Merge the reviews with recipe details based on RecipeId\nmerged_df = pd.merge(reviews_df, recipes_df, on='RecipeId')\n\n# Check the first few rows of the merged dataset\nprint(merged_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T06:09:50.170470Z","iopub.execute_input":"2024-12-10T06:09:50.171076Z","iopub.status.idle":"2024-12-10T06:09:51.967605Z","shell.execute_reply.started":"2024-12-10T06:09:50.171036Z","shell.execute_reply":"2024-12-10T06:09:51.966702Z"}},"outputs":[{"name":"stdout","text":"   ReviewId  RecipeId  AuthorId_x      AuthorName_x  Rating  \\\n0         2       992        2008         gayg msft       5   \n1         7      4384        1634     Bill Hilbrich       4   \n2         9      4523        2046  Gay Gilmore ckpt       2   \n3        13      7435        1773     Malarkey Test       5   \n4        14        44        2085        Tony Small       5   \n\n                                              Review         DateSubmitted  \\\n0       better than any you can get at a restaurant!  2000-01-25T21:44:00Z   \n1  I cut back on the mayo, and made up the differ...  2001-10-17T16:49:59Z   \n2  i think i did something wrong because i could ...  2000-02-25T09:00:00Z   \n3  easily the best i have ever had.  juicy flavor...  2000-03-13T21:15:00Z   \n4                                 An excellent dish.  2000-03-28T12:51:00Z   \n\n           DateModified                                           Name  \\\n0  2000-01-25T21:44:00Z                        Jalapeno Pepper Poppers   \n1  2001-10-17T16:49:59Z                                      Curry Dip   \n2  2000-02-25T09:00:00Z  Chinese Imperial Palace General Tso's Chicken   \n3  2000-03-13T21:15:00Z                       Kevin's Best Corned Beef   \n4  2000-03-28T12:51:00Z                         Warm Chicken A La King   \n\n   AuthorId_y  ... SaturatedFatContent CholesterolContent SodiumContent  \\\n0        1545  ...                 4.9               23.7         172.5   \n1        1920  ...                 0.0                0.0         291.3   \n2        1932  ...                 1.4              133.1        2077.6   \n3        1986  ...                14.6              222.1        2783.7   \n4        1596  ...                31.9              405.8         557.2   \n\n  CarbohydrateContent FiberContent SugarContent ProteinContent RecipeServings  \\\n0                 3.2          0.6          0.9            4.3           24.0   \n1                 0.9          0.4          0.1            0.2            4.0   \n2                45.5          1.3         20.3           43.0            8.0   \n3                41.5          9.5         11.8           47.0           12.0   \n4                29.1          3.1          5.0           45.3            2.0   \n\n  RecipeYield                                 RecipeInstructions  \n0         NaN  c(\"In a mixing bowl, combine cheeses, bacon an...  \n1         NaN  c(\"Combine all in a bowl and mix thoroughly.\",...  \n2         NaN  c(\"Combine all ingredients for sauce in a quar...  \n3         NaN  c(\"Use a 14 to 20 qt. pan.\", \"Coarsely chop en...  \n4         NaN  c(\"Melt 1 1/2 ozs butter, add the flour and co...  \n\n[5 rows x 35 columns]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Handling Missing Values\n","metadata":{}},{"cell_type":"code","source":"# Fill missing values in the 'Review' column\nmerged_df['Review'] = merged_df['Review'].fillna(\"\")\n\n# Check the columns of merged_df before dropping\nprint(merged_df.columns)\n\n# Drop irrelevant columns (handling missing ones safely)\ncolumns_to_drop = ['ReviewId', 'AuthorId', 'AuthorName', 'DateSubmitted', 'DateModified']\nmerged_df.drop(columns=[col for col in columns_to_drop if col in merged_df.columns], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T06:09:51.969933Z","iopub.execute_input":"2024-12-10T06:09:51.970596Z","iopub.status.idle":"2024-12-10T06:09:52.870966Z","shell.execute_reply.started":"2024-12-10T06:09:51.970566Z","shell.execute_reply":"2024-12-10T06:09:52.870014Z"}},"outputs":[{"name":"stdout","text":"Index(['ReviewId', 'RecipeId', 'AuthorId_x', 'AuthorName_x', 'Rating',\n       'Review', 'DateSubmitted', 'DateModified', 'Name', 'AuthorId_y',\n       'AuthorName_y', 'CookTime', 'PrepTime', 'TotalTime', 'DatePublished',\n       'Description', 'Images', 'RecipeCategory', 'Keywords',\n       'RecipeIngredientQuantities', 'RecipeIngredientParts',\n       'AggregatedRating', 'ReviewCount', 'Calories', 'FatContent',\n       'SaturatedFatContent', 'CholesterolContent', 'SodiumContent',\n       'CarbohydrateContent', 'FiberContent', 'SugarContent', 'ProteinContent',\n       'RecipeServings', 'RecipeYield', 'RecipeInstructions'],\n      dtype='object')\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Text Preprocessing for Reviews","metadata":{}},{"cell_type":"code","source":"import re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\n# Preprocessing function for the review text\ndef preprocess_text(text):\n    # Convert to lowercase\n    text = text.lower()\n    # Remove non-alphabetical characters\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    # Tokenize text\n    tokens = word_tokenize(text)\n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n    return ' '.join(tokens)\n\n# Apply preprocessing to the review text column\nmerged_df['ProcessedReview'] = merged_df['Review'].apply(preprocess_text)\n\n# Show processed reviews\nprint(merged_df[['Review', 'ProcessedReview']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T06:09:52.872228Z","iopub.execute_input":"2024-12-10T06:09:52.872944Z","iopub.status.idle":"2024-12-10T06:17:46.013038Z","shell.execute_reply.started":"2024-12-10T06:09:52.872904Z","shell.execute_reply":"2024-12-10T06:17:46.012162Z"}},"outputs":[{"name":"stdout","text":"                                              Review  \\\n0       better than any you can get at a restaurant!   \n1  I cut back on the mayo, and made up the differ...   \n2  i think i did something wrong because i could ...   \n3  easily the best i have ever had.  juicy flavor...   \n4                                 An excellent dish.   \n\n                                     ProcessedReview  \n0                              better get restaurant  \n1  cut back mayo made difference sour cream adjus...  \n2  think something wrong could taste cornstarch f...  \n3  easily best ever juicy flavorful dry vegetable...  \n4                                     excellent dish  \n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Target Variable","metadata":{}},{"cell_type":"code","source":"# The target variable is the Rating column\ny = merged_df['Rating']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T06:17:46.014598Z","iopub.execute_input":"2024-12-10T06:17:46.015155Z","iopub.status.idle":"2024-12-10T06:17:46.019076Z","shell.execute_reply.started":"2024-12-10T06:17:46.015115Z","shell.execute_reply":"2024-12-10T06:17:46.018240Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"## Text Vectorization\n","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Tokenize the processed reviews\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(merged_df['ProcessedReview'])\nsequences = tokenizer.texts_to_sequences(merged_df['ProcessedReview'])\n\n# Define the maximum length for padding\nmax_sequence_length = 100\nX_reviews = pad_sequences(sequences, maxlen=max_sequence_length)\n\n# Show the shape of the tokenized input\nprint(f\"Tokenized Review Input Shape: {X_reviews.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T06:17:46.020323Z","iopub.execute_input":"2024-12-10T06:17:46.020711Z","iopub.status.idle":"2024-12-10T06:18:53.701969Z","shell.execute_reply.started":"2024-12-10T06:17:46.020685Z","shell.execute_reply":"2024-12-10T06:18:53.701041Z"}},"outputs":[{"name":"stdout","text":"Tokenized Review Input Shape: (1401963, 100)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Numerical Features from Recipe Data","metadata":{}},{"cell_type":"code","source":"# Example: Extract relevant numerical features\nnumerical_features = merged_df[['CookTime', 'PrepTime', 'TotalTime', 'ProteinContent', 'CarbohydrateContent', 'FatContent']].copy()\n\n# Convert time durations (e.g., PT24H) into minutes\ndef time_to_minutes(time_str):\n    if pd.isna(time_str) or time_str == 'PT0':  # Handle NaN or 'PT0' edge case\n        return 0\n    \n    hours = 0\n    minutes = 0\n\n    # Remove 'PT' prefix\n    time_str = time_str[2:]\n\n    # If there are 'H' in the string, split to get hours\n    if 'H' in time_str:\n        hours = int(time_str.split('H')[0])\n\n    # If there are 'M' in the string, split to get minutes\n    if 'M' in time_str:\n        minutes = int(time_str.split('M')[0].split('H')[-1])\n\n    return hours * 60 + minutes\n\n# Apply the conversion function to the time columns\nnumerical_features['CookTime'] = numerical_features['CookTime'].apply(time_to_minutes)\nnumerical_features['PrepTime'] = numerical_features['PrepTime'].apply(time_to_minutes)\nnumerical_features['TotalTime'] = numerical_features['TotalTime'].apply(time_to_minutes)\n\n# Normalize numerical features\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_numerical = scaler.fit_transform(numerical_features)\n\nprint(f\"Numerical Features Shape: {X_numerical.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T06:18:53.703277Z","iopub.execute_input":"2024-12-10T06:18:53.703932Z","iopub.status.idle":"2024-12-10T06:18:58.530534Z","shell.execute_reply.started":"2024-12-10T06:18:53.703891Z","shell.execute_reply":"2024-12-10T06:18:58.529599Z"}},"outputs":[{"name":"stdout","text":"Numerical Features Shape: (1401963, 6)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Combine All Features","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Combine reviews and numerical features\nX_combined = [X_reviews, X_numerical]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T06:18:58.531697Z","iopub.execute_input":"2024-12-10T06:18:58.532562Z","iopub.status.idle":"2024-12-10T06:18:58.536086Z","shell.execute_reply.started":"2024-12-10T06:18:58.532533Z","shell.execute_reply":"2024-12-10T06:18:58.535325Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Building the LSTM Model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Dropout, Bidirectional\n\n# Define the review input layer (for LSTM)\nreview_input = Input(shape=(max_sequence_length,))\n\n# Define embedding and LSTM layers for reviews\nembedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length)(review_input)\nlstm_layer = Bidirectional(LSTM(100))(embedding_layer)\n\n# Define the input layer for numerical features\nnumerical_input = Input(shape=(X_numerical.shape[1],))\n\n# Combine the LSTM output with the numerical features\nmerged_input = Concatenate()([lstm_layer, numerical_input])\n\n# Add Dense layers\nx = Dense(64, activation='relu')(merged_input)\nx = Dropout(0.25)(x)\noutput = Dense(1, activation='linear')(x)  # Use 'linear' activation for regression output (rating prediction)\n\n# Create the model\nmodel = Model(inputs=[review_input, numerical_input], outputs=output)\n\n# Define the optimizer\noptimizer = Adam(learning_rate=1e-4, clipnorm=1.0)\n\n# Compile the model\nmodel.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n\n# Model summary\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T06:18:58.538621Z","iopub.execute_input":"2024-12-10T06:18:58.538871Z","iopub.status.idle":"2024-12-10T06:18:59.904042Z","shell.execute_reply.started":"2024-12-10T06:18:58.538847Z","shell.execute_reply":"2024-12-10T06:18:59.903261Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)  │ \u001b[38;5;34m32,833,400\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bidirectional       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)       │    \u001b[38;5;34m160,800\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m206\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m13,248\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)  │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32,833,400</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bidirectional       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">160,800</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">206</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">13,248</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m33,007,513\u001b[0m (125.91 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,007,513</span> (125.91 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m33,007,513\u001b[0m (125.91 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,007,513</span> (125.91 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"## Add EarlyStopping","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\n# Define the callback to save the best model\ncheckpoint_callback = ModelCheckpoint(\n    'best_model.h5',        # The file name to save the model\n    monitor='val_loss',     # The metric to monitor\n    save_best_only=True,    # Save only the best model\n    mode='min',             # 'min' means we want to minimize the validation loss\n    verbose=1               # Print a message when the model is saved\n)\n\n\nes = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n# Reduce Learning Rate on Plateau\nlrd = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=3, verbose=1, mode=\"max\", min_lr=1e-6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T06:18:59.904987Z","iopub.execute_input":"2024-12-10T06:18:59.905262Z","iopub.status.idle":"2024-12-10T06:18:59.911764Z","shell.execute_reply.started":"2024-12-10T06:18:59.905236Z","shell.execute_reply":"2024-12-10T06:18:59.910924Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"# Split the data into training and testing sets\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into training and testing sets\nX_train_reviews, X_test_reviews, X_train_numerical, X_test_numerical, y_train, y_test = train_test_split(\n    X_reviews, X_numerical, y, test_size=0.2, random_state=42)\n\n# Train the model\nhistory = model.fit(\n    [X_train_reviews, X_train_numerical],\n    y_train,\n    epochs=10,\n    batch_size=256,\n    callbacks=[es,lrd,checkpoint_callback],\n    validation_data=([X_test_reviews, X_test_numerical], y_test)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T06:18:59.912790Z","iopub.execute_input":"2024-12-10T06:18:59.913037Z","iopub.status.idle":"2024-12-10T06:42:35.522846Z","shell.execute_reply.started":"2024-12-10T06:18:59.913014Z","shell.execute_reply":"2024-12-10T06:42:35.521949Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m4382/4382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 40ms/step - loss: 2.4219 - mae: 1.0275 - val_loss: 1.1840 - val_mae: 0.6429 - learning_rate: 1.0000e-04\nEpoch 2/10\n\u001b[1m4382/4382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 40ms/step - loss: 1.3583 - mae: 0.7652 - val_loss: 1.1676 - val_mae: 0.6750 - learning_rate: 1.0000e-04\nEpoch 3/10\n\u001b[1m4382/4382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 40ms/step - loss: 1.2915 - mae: 0.7334 - val_loss: 1.1455 - val_mae: 0.6508 - learning_rate: 1.0000e-04\nEpoch 4/10\n\u001b[1m4381/4382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2236 - mae: 0.7078\nEpoch 4: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n\u001b[1m4382/4382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 40ms/step - loss: 1.2236 - mae: 0.7078 - val_loss: 1.1591 - val_mae: 0.6729 - learning_rate: 1.0000e-04\nEpoch 5/10\n\u001b[1m4382/4382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 40ms/step - loss: 1.1712 - mae: 0.6850 - val_loss: 1.1544 - val_mae: 0.6441 - learning_rate: 2.0000e-05\nEpoch 6/10\n\u001b[1m4382/4382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 40ms/step - loss: 1.1549 - mae: 0.6795 - val_loss: 1.1647 - val_mae: 0.6247 - learning_rate: 2.0000e-05\nEpoch 7/10\n\u001b[1m4381/4382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.1437 - mae: 0.6751\nEpoch 7: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n\u001b[1m4382/4382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 40ms/step - loss: 1.1437 - mae: 0.6751 - val_loss: 1.1668 - val_mae: 0.6445 - learning_rate: 2.0000e-05\nEpoch 8/10\n\u001b[1m4382/4382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 40ms/step - loss: 1.1263 - mae: 0.6695 - val_loss: 1.1618 - val_mae: 0.6198 - learning_rate: 4.0000e-06\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Evaluate the Model","metadata":{}},{"cell_type":"code","source":"# Evaluate the model\n# from tensorflow.keras.models import load_model\n\n# # Load the best saved model\n# best_model = load_model('best_model.h5')\n\n# # Evaluate or use the model for predictions\n# test_loss, test_mae = best_model.evaluate([X_test_reviews, X_test_numerical], y_test)\n# print(f\"Test Loss: {test_loss}, Test MAE: {test_mae}\")\n\ntest_loss, test_mae = model.evaluate([X_test_reviews, X_test_numerical], y_test)\nprint(f\"Test Loss: {test_loss}, Test MAE: {test_mae}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T07:04:15.144956Z","iopub.execute_input":"2024-12-10T07:04:15.145293Z","iopub.status.idle":"2024-12-10T07:04:52.051656Z","shell.execute_reply.started":"2024-12-10T07:04:15.145264Z","shell.execute_reply":"2024-12-10T07:04:52.050912Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m8763/8763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 4ms/step - loss: 1.1468 - mae: 0.6508\nTest Loss: 1.1454654932022095, Test MAE: 0.650833249092102\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# Predictions and Optimization","metadata":{}},{"cell_type":"code","source":"# Split the data into training and testing sets\nX_train_reviews, X_test_reviews, X_train_numerical, X_test_numerical, y_train, y_test = train_test_split(\n    X_reviews, X_numerical, y, test_size=0.2, random_state=42\n)\n\n# Get the indices for the test set\n_, test_indices = train_test_split(merged_df.index, test_size=0.2, random_state=42)\n\n# Generate predictions for the test set\npredictions = model.predict([X_test_reviews, X_test_numerical]).flatten()\n\n# Create a DataFrame containing the test set rows\ntest_df = merged_df.iloc[test_indices].copy()  # Use test indices to filter the original dataset\n\n# Add the predictions to the test DataFrame\ntest_df['PredictedRating'] = predictions\n\n# Filter for optimized menu (e.g., high predicted ratings and positive sentiment)\noptimized_menu = test_df[(test_df['PredictedRating'] > 4) ]\n\n# Display the optimized menu\nprint(optimized_menu[['Name', 'PredictedRating']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T06:43:12.444447Z","iopub.execute_input":"2024-12-10T06:43:12.444728Z","iopub.status.idle":"2024-12-10T06:43:53.148608Z","shell.execute_reply.started":"2024-12-10T06:43:12.444702Z","shell.execute_reply":"2024-12-10T06:43:53.147697Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m8763/8763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 4ms/step\n                                        Name  PredictedRating\n718879   Heathier Banana Pumpkin Spice Bread         4.522332\n1006375                  Roasted Green Beans         4.288333\n1296712              Parmesan Catfish Filets         4.619602\n107386            Best Rub for Grilled Steak         4.351974\n1171290                    Simple Irish Stew         4.416887\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import numpy as np\n\n# Function to preprocess the input review\ndef preprocess_input_review(review_text):\n    # Preprocess using the same steps as training\n    review_text = review_text.lower()\n    review_text = re.sub(r'[^a-zA-Z\\s]', '', review_text)\n    tokens = word_tokenize(review_text)\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n    processed_review = ' '.join(tokens)\n    return processed_review\n\n# Function to predict rating and menu inclusion\ndef predict_menu_inclusion(food_name, review_text, cook_time, prep_time, total_time, protein, carbs, fat):\n    # Preprocess the review\n    processed_review = preprocess_input_review(review_text)\n    \n    # Tokenize and pad the review\n    review_sequence = tokenizer.texts_to_sequences([processed_review])\n    padded_review = pad_sequences(review_sequence, maxlen=max_sequence_length)\n    \n    # Prepare numerical features\n    numerical_features = np.array([[time_to_minutes(cook_time), \n                                     time_to_minutes(prep_time), \n                                     time_to_minutes(total_time), \n                                     protein, carbs, fat]])\n    numerical_features = scaler.transform(numerical_features)\n    \n    # Predict the rating\n    predicted_rating = model.predict([padded_review, numerical_features]).flatten()[0]\n    \n    # Decide whether to include the item on the menu\n    include_on_menu = predicted_rating > 4  # Assuming 4 is the threshold for inclusion\n    \n    return {\n        \"FoodName\": food_name,\n        \"PredictedRating\": predicted_rating,\n        \"IncludeOnMenu\": include_on_menu\n    }\n\n# Example usage\nfood_name = \"Paneer Butter Masala\"\nreview_text = \"This dish is delicious, rich in flavor, and a favorite among customers!\"\ncook_time = \"PT30M\"  # e.g., 30 minutes\nprep_time = \"PT15M\"  # e.g., 15 minutes\ntotal_time = \"PT45M\"  # e.g., 45 minutes\nprotein = 20.0  # in grams\ncarbs = 10.0  # in grams\nfat = 15.0  # in grams\n\nresult = predict_menu_inclusion(food_name, review_text, cook_time, prep_time, total_time, protein, carbs, fat)\nprint(result)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T06:43:53.180850Z","iopub.execute_input":"2024-12-10T06:43:53.181112Z","iopub.status.idle":"2024-12-10T06:43:53.262673Z","shell.execute_reply.started":"2024-12-10T06:43:53.181087Z","shell.execute_reply":"2024-12-10T06:43:53.261818Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n{'FoodName': 'Paneer Butter Masala', 'PredictedRating': 4.5667415, 'IncludeOnMenu': True}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n  warnings.warn(\n","output_type":"stream"}],"execution_count":17}]}